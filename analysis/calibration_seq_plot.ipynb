{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "\n",
    "from dataflow.core.lispress import parse_lispress, render_compact\n",
    "from calibration_metric.vis.calibration_plot import plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = 3 \n",
    "def read_nucleus_file(miso_pred_file):\n",
    "    with open(miso_pred_file, \"r\") as f:\n",
    "        data = [json.loads(x) for x in f.readlines()]\n",
    "    to_ret = []\n",
    "    data_by_src_str = defaultdict(list)\n",
    "    for line in data:\n",
    "        data_by_src_str[line['src_str']].append(line) \n",
    "\n",
    "    for src_str, lines in data_by_src_str.items():\n",
    "        total_probs = [np.exp(np.sum(np.log(x['expression_probs']))) \n",
    "                                if x['expression_probs'] is not None else 0.0 \n",
    "                                    for x in lines ]\n",
    "        min_probs = []\n",
    "        for x in lines:\n",
    "            if x['expression_probs'] is not None and len(x['expression_probs']) > 0:\n",
    "                min_probs.append(np.min(x['expression_probs']))\n",
    "            else:\n",
    "                min_probs.append(0.0)\n",
    "\n",
    "        combo_lines = zip(lines, min_probs, total_probs)\n",
    "        sorted_combo_lines = sorted(combo_lines, key=lambda x: x[-1], reverse=True)\n",
    "        data_by_src_str[src_str] = sorted_combo_lines\n",
    "    return data_by_src_str\n",
    "\n",
    "def read_gold_file(file):\n",
    "    with open(file) as f:\n",
    "        if file.endswith(\".tgt\"):\n",
    "            to_ret = [render_compact(parse_lispress(line)) for line in f.readlines()]\n",
    "        else:\n",
    "            to_ret = [re.sub(\"__StartOfProgram\", \"\", x).strip() for x in f.readlines()]\n",
    "    return to_ret \n",
    "\n",
    "def get_probs_and_accs(nucleus_file, gold_src_file, gold_tgt_file):\n",
    "    nucleus = read_nucleus_file(nucleus_file)\n",
    "    gold_tgt = read_gold_file(gold_tgt_file)\n",
    "    gold_src = read_gold_file(gold_src_file)\n",
    "    # assert(len(nucleus) == len(gold_tgt))\n",
    "    probs = []\n",
    "    accs = []\n",
    "    for i,  (gold_src, gold_tgt) in enumerate(zip(gold_src, gold_tgt)):\n",
    "        nuc = nucleus[gold_src]\n",
    "        nuc_str = nuc[0][0]['tgt_str']\n",
    "            \n",
    "        nuc_str = render_compact(parse_lispress(nuc_str))\n",
    "        # use the min prob, not the summed prob \n",
    "        probs.append(nuc[0][1])\n",
    "        accs.append(nuc_str == gold_tgt)\n",
    "    return probs, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, accs = get_probs_and_accs(\"/brtx/604-nvme1/estengel/calflow_calibration/miso/tune_roberta_tok_fix_benchclamp_data/translate_output_calibrated/dev_all.tgt\",\n",
    "                                \"/brtx/601-nvme1/estengel/resources/data/smcalflow.agent.data.from_benchclamp/dev_all.src_tok\",\n",
    "                                \"/brtx/601-nvme1/estengel/resources/data/smcalflow.agent.data.from_benchclamp/dev_all.tgt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bins_to_df(\n",
    "    values: np.array,\n",
    "    bin_edges: np.array,\n",
    "    bin_number: np.array,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the output of bin_preds to a pandas dataframe.\n",
    "    DataFrame has following columns:\n",
    "    - prob_model: the probability for the bin\n",
    "    - prob_correct: the average number of correct examples in the bin\n",
    "    - count: the number of examples in the bin\n",
    "    \"\"\"\n",
    "    # create LUT for bin number to number of items in that bin \n",
    "    bin_lookup = Counter(bin_number)\n",
    "    # instantiate df \n",
    "    # df = pd.DataFrame(columns=[\"prob_model\", \"prob_correct\", \"count\"])\n",
    "    # populate df\n",
    "    df_data = []\n",
    "    for i, (val, edge_start, bin_num) in enumerate(zip(values, bin_edges, bin_number)):\n",
    "        edge_end = bin_edges[i+1]\n",
    "        midpoint = (edge_start + edge_end) / 2\n",
    "        df_data.append({\"prob_model\": midpoint, \n",
    "                        \"prob_correct\": val, \n",
    "                        \"count\": bin_lookup[i+1]})\n",
    "    df = pd.DataFrame.from_dict(df_data)\n",
    "    df['normalized_count'] = df['count'] / df['count'].sum()\n",
    "    df['log_count'] = np.log(df['count']) \n",
    "    # NOTE: this is not the same as the log of the normalized count; it is intended to\n",
    "    # discount high count bins.\n",
    "    df['normalized_log_count'] = df['log_count'] / df['log_count'].sum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(values, \n",
    "bins, \n",
    "bin_number) = stats.binned_statistic(\n",
    "    probs, \n",
    "    accs, \n",
    "    statistic='mean', \n",
    "    bins=10)\n",
    "\n",
    "df_to_plot = bins_to_df(values, bins, bin_number)\n",
    "\n",
    "plot = plot_df(df_to_plot, ylabel=\"Minimum Model Prob.\", xlabel=\"Exact Match Accuracy\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(12)\n",
    "# stratified sample \n",
    "# bin_number has bin assignment \n",
    "# values has the mean acc of the bin \n",
    "# bins has the bin edges \n",
    "\n",
    "examples_by_bin = defaultdict(list)\n",
    "idxs_by_bin_number = defaultdict(list)\n",
    "for i, bin_num in enumerate(bin_number):\n",
    "    idxs_by_bin_number[bin_num].append(i)\n",
    "\n",
    "\n",
    "for i, (acc, edge_start) in enumerate(zip(values, bins)):\n",
    "    edge_end = bins[i+1]\n",
    "    midpoint = (edge_start + edge_end) / 2\n",
    "    midpoint = f\"{midpoint:.2f}\"\n",
    "    examples_by_bin[midpoint] = idxs_by_bin_number[i+1]\n",
    "\n",
    "gold_src = read_gold_file(\"/brtx/601-nvme1/estengel/resources/data/smcalflow.agent.data.from_benchclamp/dev_all.src_tok\")\n",
    "gold_tgt = read_gold_file(\"/brtx/601-nvme1/estengel/resources/data/smcalflow.agent.data.from_benchclamp/dev_all.tgt\") \n",
    "nucleus = read_nucleus_file(\"/brtx/604-nvme1/estengel/calflow_calibration/miso/tune_roberta_tok_fix_benchclamp_data/translate_output_calibrated/dev_all.tgt\")\n",
    "\n",
    "\n",
    "# we want 100 total examples, 10 per bin \n",
    "n_samples_per_bin = 10 \n",
    "\n",
    "sampled_source = defaultdict(list)\n",
    "sampled_target = defaultdict(list)\n",
    "sampled_nucleus = defaultdict(list)\n",
    "sampled_idx_dict = defaultdict(list)\n",
    "for midpoint, idxs in examples_by_bin.items():\n",
    "    # reduce to idxs that are not fences \n",
    "    before_len = len(idxs)\n",
    "    idxs = [i for i in idxs if \"Fence\" not in gold_tgt[i] and \"Pleasantry\" not in gold_tgt[i]]\n",
    "    after_len = len(idxs)\n",
    "    assert(before_len > after_len)\n",
    "\n",
    "    sampled_idxs = np.random.choice(idxs, size=n_samples_per_bin, replace=False)\n",
    "    for idx in sampled_idxs:\n",
    "        sampled_source[midpoint].append(gold_src[idx])\n",
    "        sampled_target[midpoint].append(gold_tgt[idx])\n",
    "        nucleus_list = nucleus[gold_src[idx]]\n",
    "        sampled_nucleus[midpoint].append(nucleus_list)\n",
    "        sampled_idx_dict[midpoint].append(idx)\n",
    "\n",
    "# write tgt \n",
    "out_file = \"../hit/data/for_hit_round_2/stratified_data_by_bin.tgt\"\n",
    "tgt_out_file = \"../hit/data/for_hit_round_2/gold_data/stratified_data_by_bin.tgt\"\n",
    "idx_out_file = \"../hit/data/for_hit_round_2/gold_data/stratified_data_by_bin.idx\"\n",
    "src_out_file = \"../hit/data/for_hit_round_2/gold_data/stratified_data_by_bin.src_tok\"\n",
    "bin_out_file = \"../hit/data/for_hit_round_2/gold_data/stratified_data_by_bin.bins\"\n",
    "with open(out_file, \"w\") as f1, \\\n",
    "    open(tgt_out_file, \"w\") as tgtf, \\\n",
    "    open(src_out_file, \"w\") as srcf, \\\n",
    "    open(idx_out_file, \"w\") as idxf, \\\n",
    "    open(bin_out_file, \"w\") as binf:\n",
    "    for midpoint, nuc_list in sampled_nucleus.items():\n",
    "        for nuc_sublist in nuc_list:\n",
    "            for nuc, min_prob, total_prob in nuc_sublist:\n",
    "                nuc['midpoint'] = float(midpoint) \n",
    "                nuc['min_prob'] = min_prob\n",
    "                nuc = json.dumps(nuc)\n",
    "                f1.write(nuc + \"\\n\")\n",
    "            binf.write(str(midpoint) + \"\\n\")\n",
    "        tgt_str_list = sampled_target[midpoint]\n",
    "        for tgt_str in tgt_str_list:\n",
    "            tgtf.write(tgt_str.strip() + \"\\n\")\n",
    "\n",
    "        src_str_list = sampled_source[midpoint]\n",
    "        for src_str in src_str_list:\n",
    "            srcf.write(src_str.strip() + \"\\n\")\n",
    "        idx_list = sampled_idx_dict[midpoint]\n",
    "        for idx in idx_list:\n",
    "            idxf.write(str(idx) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we've run the HIT, let's analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/estengel/incremental-function-learning/\")\n",
    "sys.path.insert(0, \"/home/estengel/incremental-function-learning/hit\")\n",
    "sys.path.insert(0, \"/home/estengel/incremental-function-learning/hit/scripts\")\n",
    "from hit.results.analyze_csv import run_choose_and_rewrite, read_json, read_csv, clean_lispress\n",
    "import pdb \n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "turk_file = \"../hit/results/round2_incomplete/combined.csv\"\n",
    "json_file = \"../hit/data/for_hit/from_stratified/for_turk/data.json\"\n",
    "\n",
    "turk_data = read_csv(turk_file)\n",
    "json_data = read_json(json_file)\n",
    "\n",
    "class Args:\n",
    "    checkpoint_dir = \"/brtx/604-nvme1/estengel/calflow_calibration/miso/tune_roberta_tok_fix_benchclamp_data\"\n",
    "    aggregator = \"none\"\n",
    "    n_redundant = 1\n",
    "\n",
    "args = Args()\n",
    "non_rewritten_data, rewritten_data = run_choose_and_rewrite(turk_data, json_data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (> (size (QueryEventResponse.results (FindEventWrapperWithDefaults (& (Event.attendees_? (AttendeeListHasRecipientConstraint (RecipientWithNameLike (^(Recipient) EmptyStructConstraint) (PersonName.apply \" Julie \")))) (EventDuringRange (^(Event) EmptyStructConstraint) (SeasonFall)))))) 0L))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (DeleteCommitEventWrapper (DeletePreflightEventWrapper (Event.id (singleton (QueryEventResponse.results (FindEventWrapperWithDefaults (EventOnDateTime (DateAtTimeWithDefaults (NextDOW (Sunday)) (HourMilitary 7L)) (Event.subject_? (?~= \" service \"))))))))))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(let (x0 (singleton (QueryEventResponse.results (FindEventWrapperWithDefaults (Event.subject_? (?~= \" Car Seats \")))))) (Yield (UpdateCommitEventWrapper (UpdatePreflightEventWrapper (Event.id x0) (Event.start_? (DateTime.time_? (?= (ConvertTimeToPM (DateTime.time (Event.start x0))))))))))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (Execute (ReviseConstraint (refer (^(Dynamic) roleConstraint (Path.apply \" output \"))) (^(Event) ConstraintTypeIntension) (EventOnDate (ClosestDayOfWeek (Execute (refer (extensionConstraint (^(Date) EmptyStructConstraint)))) (Sunday)) (^(Event) EmptyStructConstraint)))))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (Execute (ReviseConstraint (refer (^(Dynamic) roleConstraint (Path.apply \" output \"))) (^(Event) ConstraintTypeIntension) (EventOnDate (NextDOW (Thursday)) (^(Event) EmptyStructConstraint)))))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (Event.duration (singleton (QueryEventResponse.results (FindEventWrapperWithDefaults (EventOnDate (NextDOW (Thursday)) (Event.subject_? (?~= \" work lunch \"))))))))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (CreateCommitEventWrapper (CreatePreflightEventWrapper (& (& (Event.subject_? (?= \" drinks \")) (Event.start_? (?= (DateAtTimeWithDefaults (Tomorrow) (NumberPM 8L))))) (Event.duration_? (?= (toHours 1.5)))))))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (UpdateCommitEventWrapper (UpdatePreflightEventWrapper (Event.id (singleton (QueryEventResponse.results (FindEventWrapperWithDefaults (Event.subject_? (?~= \" dancing competition \")))))) (Event.start_? (DateTime.time_? (?= (NumberPM 8L)))))))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (Execute (ChooseCreateEvent (BottomResult) (refer (^(Dynamic) ActionIntensionConstraint)))))\n",
      "dict_keys(['tgt_str', 'expression_probs', 'src_str', 'midpoint', 'min_prob'])\n",
      "(Yield (DeleteCommitEventWrapper (DeletePreflightEventWrapper (Event.id (singleton (QueryEventResponse.results (FindEventWrapperWithDefaults (EventOnDate (Tomorrow) (Event.attendees_? (AttendeeListHasRecipientConstraint (RecipientWithNameLike (^(Recipient) EmptyStructConstraint) (PersonName.apply \" Kim Possible \"))))))))))))\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "\n",
    "non_rewritten_correct_to_plot = []\n",
    "hit_dict = defaultdict(list)\n",
    "for nr in non_rewritten_data:\n",
    "    hit_dict[nr['json_entry']['bin']].append(int(nr['is_correct_most_likely']))\n",
    "\n",
    "# we can add rewritten to the same plot\n",
    "for r in rewritten_data:\n",
    "    hit_dict[r['json_entry']['bin']].append(int(r['is_correct'])) \n",
    "\n",
    "bins = []\n",
    "ys_hit = []\n",
    "ys_nucleus = []\n",
    "for bin, is_correct_list in hit_dict.items():\n",
    "    bins.append(bin)\n",
    "    ys_hit.append(np.mean(is_correct_list))\n",
    "\n",
    "# TODO (elias): get the line from nucleus data for this bin, then get the accuracy, so we can plot \n",
    "# the same examples as were run in the HIT \n",
    "ys_nuc = defaultdict(list)\n",
    "nucleus_data_by_src = read_nucleus_file(\"../hit/data/for_hit_round_2/stratified_data_by_bin.tgt\")\n",
    "tgt_data = read_gold_file(\"../hit/data/for_hit_round_2/gold_data/stratified_data_by_bin.tgt\")\n",
    "src_data = read_gold_file(\"../hit/data/for_hit_round_2/gold_data/stratified_data_by_bin.src_tok\")\n",
    "tgt_data_by_src = {src: tgt for src, tgt in zip(src_data, tgt_data)}\n",
    "\n",
    "for bin in hit_dict.keys(): \n",
    "    for src, nuc_list in nucleus_data_by_src.items():\n",
    "        nuc, min_prob, total_prob = nuc_list[0]\n",
    "\n",
    "        if nuc['midpoint'] == bin:\n",
    "            gold_tgt = tgt_data_by_src[src]\n",
    "            pred_tgt = nuc['tgt_str']\n",
    "            if gold_tgt == pred_tgt:\n",
    "                ys_nuc[bin] = 1\n",
    "            print(gold_tgt)\n",
    "            break\n",
    "            # ys_nuc[bin] = nuc['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# swap df \n",
    "swapped_df = df_to_plot.copy()\n",
    "swapped_df['prob_correct'] = df_to_plot['prob_model']\n",
    "swapped_df['prob_model'] = df_to_plot['prob_correct']\n",
    "\n",
    "plot = plot_df(swapped_df, xlabel=\"Minimum Model Prob.\", ylabel=\"Exact Match Accuracy\", ax = ax) \n",
    "\n",
    "xys = list(zip(xs, ys))\n",
    "# sort by bin \n",
    "xys = sorted(xys, key=lambda x: x[0])\n",
    "xs, ys = zip(*xys)\n",
    "print(\"xs\", xs)\n",
    "print(\"ys\", ys)\n",
    "# now we've swapped to have accuracy on the y axis\n",
    "ax.plot(xs, ys, label=\"Non-Rewritten\", color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('miso_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9db1c0e1c59df8b57e6ccdbb7c3746bf45f553f1d28626d748c067e213df640a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
